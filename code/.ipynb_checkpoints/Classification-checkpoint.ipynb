{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f30487ab-c2a7-4231-80cd-942fbc97e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76418ef-1f2e-4e21-b593-8aca2922d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_src = os.path.abspath(\"D:\\Documenti\\GitHub\\Advanced_DL\\datasets\\Fer2013\\\\train\")\n",
    "ds_test_src = os.path.abspath(\"D:\\Documenti\\GitHub\\Advanced_DL\\datasets\\Fer2013\\\\test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f983610-8ba7-4afc-81b4-44c255c5257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ImageFolder(root=ds_train_src, transform=ToTensor())\n",
    "df_test = ImageFolder(root=ds_test_src, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f96e28-c09a-4c5f-836b-51ea2ae0ed52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = df_train.classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fab602f-e6fa-46f2-898c-ad2d697ed41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(df_train, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(df_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507d69b6-d282-4f45-88a6-efff99d18dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 3, 48, 48])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_dataloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd3877-6f5a-41a3-9bbe-8351e63f854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddbdb78b-7166-4cbd-823c-247d13ae6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'val_loss': 2, 'val_acc': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeb0d813-d94b-4b08-aca1-198d85119b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[list(res.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9143406a-6f11-42b7-83c4-b6c671750901",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977248f-bcac-4d98-806b-013f3fa7e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def __init__(self, loss_function, metrics):\n",
    "        super().__init__()\n",
    "        self.loss_function = loss_function\n",
    "        self.metrics = metrics\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = self.loss_function(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = self.loss_function(out, labels)   # Calculate loss\n",
    "        result = {'val_loss': loss.detach()}\n",
    "        \n",
    "        for m in self.metrics:\n",
    "            result[m.name] = m.eval(out, labels)           # Calculate metrics\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        \n",
    "        result = {'val_loss': epoch_loss.item()}\n",
    "        \n",
    "        for m in self.metrics:\n",
    "            batch = [x[m.name] for x in outputs]\n",
    "            epoch = torch.stack(batch).mean()      # Combine metrics\n",
    "            result[m.name] = epoch.item()\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        out = f\"Epoch [{epoch}]\"\n",
    "        vals = list(res.keys())\n",
    "        for v in vals:\n",
    "            out += f\"{v}: {result[v]:.4f}\"\n",
    "        print(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042dd71-2d30-4b9e-b7fd-dc0178421675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def eval(self, outputs, labels):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab563e9-aeac-45c8-a5da-e99f8cbc54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(Metric):\n",
    "    \n",
    "    def eval(self, outputs, labels):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e853a8-f208-4eb6-ab51-8e8d65b0e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "125ec41f-92e5-4844-bc89-6eeb84c08eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5, stride=1) # (12, 44, 44 )\n",
    "        self.bn1 = nn.BatchNorm2d(12)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5, stride=1) # (12, 40, 40 )\n",
    "        self.bn2 = nn.BatchNorm2d(12)\n",
    "        self.pool = nn.MaxPool2d(2,2)                                                    # (12, 20, 20 )\n",
    "        self.conv4 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5, stride=1) # (24, 16, 16 )\n",
    "        self.bn4 = nn.BatchNorm2d(24)\n",
    "        self.conv5 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, stride=1) # (24, 12, 12 )\n",
    "        self.bn5 = nn.BatchNorm2d(24)\n",
    "        self.fc1 = nn.Linear(24*12*12, len(classes))\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = F.relu(self.bn1(self.conv1(input)))      \n",
    "        output = F.relu(self.bn2(self.conv2(output)))     \n",
    "        output = self.pool(output)                        \n",
    "        output = F.relu(self.bn4(self.conv4(output)))     \n",
    "        output = F.relu(self.bn5(self.conv5(output)))    \n",
    "        output = output.view(-1, 24*12*12)\n",
    "        output = self.fc1(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206f37c4-3a5d-4096-a1c9-5212bdf059cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f357986-b273-4935-bbb9-562e4726f03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(12, 24, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn5): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=3456, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff063a8-8125-4c98-a70f-c3b60c2ce7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435e0eed8fc74b1e82d3a03251c50bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2nd loop: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_dataloader, 0),desc='2nd loop'):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1403cef9-c12a-4e66-9565-7f8b7eaf2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_dataloader):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b7789-29fe-43c3-9821-11cb5215fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_dataloader):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767bda14-919f-42f6-940e-7a31ab451993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
